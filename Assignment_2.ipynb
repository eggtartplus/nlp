{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eggtartplus/nlp/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "In this assignment, we will focus on regular expressions and byte-pair encoding. Assignment 2 is worth 8% toward your final grade.\n",
        "\n",
        "Should you have any inquiries, please feel free to ask in the GitHub discussion forum found at: https://github.com/orgs/SLPcourse/discussions/categories/assignments.\n",
        "\n",
        "Please follow the template to finish your assignment and submit in GitHub.\n"
      ],
      "metadata": {
        "id": "f6iyutoXsYxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [50 marks] Regular expression\n",
        "\n",
        "1. [30marks] Write python code with regular expressions to clean the html webpage.\n",
        "\n",
        "  Input: https://slpcourse.github.io/materials/html_page.txt\n",
        "\n",
        "  Reference Output: https://slpcourse.github.io/materials/reference.txt\n",
        "\n",
        "2. [20 marks] Based on the output from 1, extract the lines with lecture time, tutorial time and office hours. Your need to use regular expressions.\n",
        "\n",
        "\n",
        "Note: We expect the regular expressions to be just enough for the task. If there are extra non-used regular expressions, we will deduct scores based the lines of non-used regular expressions. Each line that contains non-used regular expressions is worth 5 marks. Each uncleaned html tag is worth 2 points. Each unnecessary whitespace is worth 1 point."
      ],
      "metadata": {
        "id": "TT3nsKxz2UbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "import re\n",
        "import requests\n",
        "\n",
        "url = \"https://slpcourse.github.io/materials/html_page.txt\"\n",
        "r = requests.get(url)\n",
        "filename = \"html_page.txt\"\n",
        "with open(filename, 'wb') as f:\n",
        "  f.write(r.content)\n",
        "with open(filename,'r') as rf:\n",
        "    con = rf.read()\n",
        "\n",
        "# removing recurring tags, like <b>,</b>,<a>,</a>\n",
        "c = re.sub(r'(<b>|<\\/b>|<\\/a>|<a(.|\\n)*?>)','',con)\n",
        "# spliting the file using <>\n",
        "c = re.split(r\"<[^<>]*>|<!--.*-->\",c)\n",
        "# removing blank\n",
        "c = [i.strip() for i in c]   #remove leading and trailing spaces\n",
        "c = [re.sub(r'[\\n\\t]+', ' ', i) for i in c]\n",
        "c = list(filter(None, c))   #filter the \"\"\n",
        "\n",
        "final = '\\n'.join(c)\n",
        "print(final)"
      ],
      "metadata": {
        "id": "FjKVgSWFtmwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abec7ee-fef0-4434-f332-d1d411df039f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSC3160 Fundamentals of Speech and Language Processing\n",
            "MDS6002 Natural Language Processing\n",
            "Spring 2023\n",
            "The difference between speech and language processing and other data processing is the use of knowledge of language. In this course, we will study how to describe, process and compute different levels of language knowledge including Phonetics and Phonology, Morphology, Syntax, Semantics, and how the language knowledge is used in speech and language applications such as named entities recognition, information extraction, question answering, speech recognition, and speech synthesis.\n",
            "Teaching team\n",
            "Instructor\n",
            "Zhizheng Wu\n",
            "TA\n",
            "Xi Chen\n",
            "TA\n",
            "Xueyao Zhang\n",
            "Poster Session\n",
            "A final project poster session is planned by the end of the course (tentatively May 20th 2023). This is to provide students the opportunities to connect with speech and language research/industry community.\n",
            "Anyone from the CUHK-Shenzhen and speech and language technology community are welcome to join. More details will be provided when it is close to the event. Feel free to reach out!\n",
            "Logistics\n",
            "Lectures: are on Tuesday/Thursday 4:00PM - 5:20PM in TB103. Note: lectures will be remote for the first two weeks, and hybrid afterwards. The Zoom link is posted on BB.\n",
            "Office hours\n",
            "Zhizheng Wu: Thu 2:30-3:30 PM. Daoyuan Building 321b\n",
            "Xi Chen: Wed 7-9PM. SDS Research Lab (4th Floor, Zhi Xin Building) Seat No.100\n",
            "Xueyao Zhang: Wed 7-9PM. SDS Research Lab (4th Floor, Zhi Xin Building) Seat No.78\n",
            "Contact: If you have any question, please reach out to us via email or post it to BB.\n",
            "Course Information\n",
            "This course is designed as the first course for students who are interested in speech and language technology. The first half of the course focuses on the fundamentals and introduces tools for students to use, and the second half emphasises on applications, giving students the opportunity to know how speech and language technology could impact human life. In particular, the topics include:\n",
            "Understanding human speech: spectrogram, fundamental frequency, formant, etc\n",
            "Human sounds and their organization\n",
            "Words and their relationship to other words\n",
            "Syntax: Structure of sentences\n",
            "Text processing and regular expressions\n",
            "Language models\n",
            "Embedding: Representations of the meaning of words\n",
            "Word classifications and Named entities recognition\n",
            "Applications: speech recognition, speech synthesis, machine translation, chatbot, etc\n",
            "Prerequisites\n",
            "Proficiency in LaTex:  All the reports need to be written by using LaTex. A template will be provided. If you are not familiar with LaTex, please learn from the tutorial in advance.\n",
            "Proficiency in GitHub:  All the source codes need to be submitted in GitHub.\n",
            "Proficiency in Python: All the assignments will be in Python (using Numpy and PyTorch).\n",
            "Basic machine learning knowledge:  It is possible to take this course without any machine learning knowledge, however, the course will be easier if you have foundations of machine learning.\n",
            "Basic Concepts of probability:  It will be easier for you to understand some lectures if you know basics of probability.\n",
            "Textbooks\n",
            "Recommended Books:\n",
            "Speech and Language Processing (3rd ed. draft), by Dan Jurafsky and James H. Martin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "con1 = re.sub(r'(<b>|<\\/b>|<a>|<\\/a>)','',con)\n",
        "pattern_tut = re.compile('.*\\d+[:-]\\d+.*\\n')  # 筛选出所有带有时间的句子，7-9,2:30等 \n",
        "tut = pattern_tut.findall(con1,re.I|re.S|re.M)\n",
        "for i in tut:\n",
        "  new = re.sub(r'(<li>|</li>)','',i)\n",
        "  print(new.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQpVGRVJggkG",
        "outputId": "6624bbf4-79df-4644-b573-51fb07d8e435"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lectures: are on Tuesday/Thursday 4:00PM - 5:20PM in TB103. Note: lectures will be remote\n",
            "Zhizheng Wu: Thu 2:30-3:30 PM. Daoyuan Building 321b\n",
            "Xi Chen: Wed 7-9PM. SDS Research Lab (4th Floor, Zhi Xin Building) Seat No.100\n",
            "Xueyao Zhang: Wed 7-9PM. SDS Research Lab (4th Floor, Zhi Xin Building) Seat No.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [50 marks] Byte-pair encoding\n",
        "\n",
        "In Lecture 6, we talked about Byte-Pair Encoding (BPE). In this assignment, the task is to implement a byte-pair encoding algorithm to learn subword tokens.\n",
        "\n",
        "Here is a vocabulary with frequency, \n",
        "```\n",
        "5 oneumonoultramicroscopicsilicovolcanoconiosis\n",
        "4 hippopotomonstrosesquippedaliophobia\n",
        "3 supercalifragilisticexpialidocious\n",
        "2 pseudopseudohypoparathyroidism\n",
        "1 floccinaucinihilipilification\n",
        "2 antidisestablishmentarianism\n",
        "3 honorificabilitudinitatibus\n",
        "```\n",
        "The first column represents the occurency frequency, and the second column represents the words.\n",
        "\n",
        "In the implementation, k is set to be 5. "
      ],
      "metadata": {
        "id": "8TD1kIfo1emC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "import re, collections\n",
        "\n",
        "# From the current training data, calculate the frequency of all consecutive character pairs\n",
        "def get_stats(vocab):\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "      symbols = word.split()\n",
        "      for i in range(len(symbols)-1):\n",
        "          pairs[symbols[i],symbols[i+1]] += freq\n",
        "  return pairs\n",
        "\n",
        "# Use the character pair with the highest frequency to merge the current training data\n",
        "def merge_vocab(pair, v_in):\n",
        "  v_out = {}\n",
        "  bigram = re.escape(' '.join(pair))\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "  for word in v_in:\n",
        "      w_out = p.sub(''.join(pair), word)\n",
        "      v_out[w_out] = v_in[word]\n",
        "  return v_out\n",
        "\n",
        "# Treat the special tag '</w>' as a character to calculate the character length\n",
        "def len_token(token):\n",
        "  if token[-4:] == '</w>':\n",
        "    return len(token[:-4])+1\n",
        "  else:\n",
        "    return len(token)\n",
        "\n",
        "# Generate the current bpe vocabulary based on the current training data\n",
        "def get_tokens(vocab):\n",
        "  bpe_vocab = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "      for symbol in word.split():\n",
        "          bpe_vocab[symbol] += freq\n",
        "  bpe_vocab = dict(sorted(bpe_vocab.items(), key=lambda x: len_token(x[0]), reverse = True))\n",
        "  return bpe_vocab\n",
        "\n",
        "\n",
        "vocab = {'o n e u m o n o u l t r a m i c r o s c o p i c s i l i c o v o l c a n o c o n i o s i s </w>':5, \n",
        "         'h i p p o p o t o m o n s t r o s e s q u i p p e d a l i o p h o b i a </w>':4,\n",
        "         's u p e r c a l i f r a g i l i s t i c e x p i a l i d o c i o u s </w>':3, \n",
        "         'p s e u d o p s e u d o h y p o p a r a t h y r o i d i s m </w>':2, \n",
        "         'f l o c c i n a u c i n i h i l i p i l i f i c a t i o n </w>':1, \n",
        "         'a n t i d i s e s t a b l i s h m e n t a r i a n i s m </w>':2, \n",
        "         'h o n o r i f i c a b i l i t u d i n i t a t i b u s </w>':3}\n",
        "\n",
        "for i in range(5):    # merge times is 5\n",
        "  pairs = get_stats(vocab)\n",
        "  best = max(pairs, key = pairs.get)\n",
        "  if pairs[best] < 2:\n",
        "    print('Character pair occurrence frequency is less than 2')\n",
        "    break\n",
        "  vocab = merge_vocab(best, vocab)\n",
        "  bpe_vocab = get_tokens(vocab)\n",
        "\n",
        "print('new training data:{}'.format(vocab))\n",
        "print('newly generated vocabulary:{}'.format(bpe_vocab.keys()))"
      ],
      "metadata": {
        "id": "_L6aAKCs2Rdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae475e8-3c13-4e62-adb5-39326e60e991"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new training data:{'on e u m on o u l t r a m ic r os c op ic s i li c o v o l c a n o c on i os i s </w>': 5, 'h i p p op o t o m on s t r os e s q u i p p e d a li op h o b i a </w>': 4, 's u p e r c a li f r a g i li s t ic e x p i a li d o c i o u s </w>': 3, 'p s e u d op s e u d o h y p op a r a t h y r o i d i s m </w>': 2, 'f l o c c i n a u c i n i h i li p i li f ic a t i on </w>': 1, 'a n t i d i s e s t a b li s h m e n t a r i a n i s m </w>': 2, 'h on o r i f ic a b i li t u d i n i t a t i b u s </w>': 3}\n",
            "newly generated vocabulary:dict_keys(['on', 'ic', 'os', 'op', 'li', 'e', 'u', 'm', 'o', 'l', 't', 'r', 'a', 'c', 's', 'i', 'v', 'n', '</w>', 'h', 'p', 'q', 'd', 'b', 'f', 'g', 'x', 'y'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uSS6_u76Gv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}